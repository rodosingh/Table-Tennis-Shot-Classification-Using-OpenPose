{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "inside-dublin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, StratifiedKFold\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-destiny",
   "metadata": {},
   "source": [
    "<h3>Import Classifiers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "selective-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-refrigerator",
   "metadata": {},
   "source": [
    "# Data Extraction = Key Points Extraction from Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-information",
   "metadata": {},
   "source": [
    "### Body Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sitting-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Body parts for COCO data-set\n",
    "\n",
    "BODY_PARTS_C = { \"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "                   \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "                   \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n",
    "                   \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18 }\n",
    "\n",
    "POSE_PAIRS_C = [ [\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
    "               [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"],\n",
    "               [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"], [\"Neck\", \"LHip\"],\n",
    "               [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"], [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"],\n",
    "               [\"REye\", \"REar\"], [\"Nose\", \"LEye\"], [\"LEye\", \"LEar\"] ]\n",
    "# required pairs for our purpose...\n",
    "POSE_PAIRS_C_new = [ [\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
    "               [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"],\n",
    "               [\"Neck\", \"RHip\"], [\"Neck\", \"LHip\"], [\"Neck\", \"Nose\"] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-waterproof",
   "metadata": {},
   "source": [
    "### Implementing in COCO datastyle format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "prompt-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tup_unpack(tup): return [tup[0], tup[1]]\n",
    "\n",
    "def frames_unraveler(points_lst):\n",
    "    data = None\n",
    "    for i in points_lst:\n",
    "        data_new = [tup_unpack(j) if j is not None else tup_unpack((np.nan,np.nan)) for j in i]\n",
    "        data_new = np.array(data_new, dtype = np.float32).ravel()\n",
    "        if data is None:\n",
    "            data = data_new\n",
    "        else:\n",
    "            data = np.vstack((data, data_new))\n",
    "    return data\n",
    "\n",
    "def imputer(points_lst):\n",
    "    data = frames_unraveler(points_lst)\n",
    "    #import sklearn.preprocessing.Impute\n",
    "    imp = SimpleImputer(missing_values = np.nan, strategy = \"most_frequent\")#\"mean\"\n",
    "    imp.fit(data); data = imp.transform(data)\n",
    "    return data.reshape((1, -1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "infrared-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_points_generator(folder_path, video_side, BODY_PARTS = BODY_PARTS_C):\n",
    "    ## Let's Perform it on video.\n",
    "    data_for_folder = None# it will contain derived data from videos in a particular folder\n",
    "    net  = cv.dnn.readNetFromTensorflow(\"graph_opt.pb\")\n",
    "    for video_name in os.listdir(folder_path):\n",
    "        cap = cv.VideoCapture(folder_path + \"/\"+video_name)\n",
    "        cap.set(cv.CAP_PROP_FPS, 10)\n",
    "        cap.set(3, 800)\n",
    "        cap.set(4, 800)\n",
    "        # 'Threshold value for pose parts heat map'\n",
    "        thr = 0.2#<-----------------------------------------------------------\n",
    "        # 'Resize input to specific width.'\n",
    "        width = 368\n",
    "        # 'Resize input to specific height.'\n",
    "        height = 368\n",
    "        # if video is not opened\n",
    "        if not cap.isOpened():\n",
    "            cap = cv.VideoCapture(0)\n",
    "        if not cap.isOpened():\n",
    "            raise IOError(\"Cannot open video\")\n",
    "            \n",
    "        flag = True; points_lst = []; ctr = 0\n",
    "        while cv.waitKey(1) < 0 and ctr < 20:\n",
    "            hasFrame, frame = cap.read(); ctr += 1\n",
    "            if not hasFrame:\n",
    "                cv.waitKey()\n",
    "                break\n",
    "            #if cv.getWindowProperty('crop_frame', cv.WND_PROP_VISIBLE) < 1:\n",
    "            #    break\n",
    "            if cv.waitKey(10) & 0xFF == ord('q') :\n",
    "                # break out of the while loop\n",
    "                break\n",
    "            if video_side == \"left\":\n",
    "                crop_frame = frame[:, 0:700, :]\n",
    "            else:\n",
    "                crop_frame = frame[:, 700:, :]\n",
    "            crop_frameWidth = crop_frame.shape[1]\n",
    "            crop_frameHeight = crop_frame.shape[0]\n",
    "            inp = cv.dnn.blobFromImage(crop_frame, 1.0, (width, height), (127.5, 127.5, 127.5), swapRB=False, crop=False)#inScale\n",
    "            net.setInput(inp)\n",
    "            out = net.forward()\n",
    "            out = out[:, :19, :, :]\n",
    "\n",
    "            #assert(len(BODY_PARTS) <= out.shape[1])\n",
    "\n",
    "            points = []\n",
    "            required_body_points = [0, 1, 2, 3, 4, 5, 6, 7, 8, 11]# see BODY_PARTS_M for reference...\n",
    "            for i in required_body_points:\n",
    "                # Slice heatmap of corresponding body's part.\n",
    "                heatMap = out[0, i, :, :]\n",
    "\n",
    "                # Originally, we try to find all the local maximums. To simplify a sample\n",
    "                # we just find a global one. However only a single pose at the same time\n",
    "                # could be detected this way.\n",
    "                _, conf, _, point = cv.minMaxLoc(heatMap)\n",
    "                x = (crop_frameWidth * point[0]) / out.shape[3]\n",
    "                y = (crop_frameHeight * point[1]) / out.shape[2]\n",
    "\n",
    "                # Add a point if it's confidence is higher than threshold.\n",
    "                points.append((int(x), int(y)) if conf > thr else None)\n",
    "            points_lst.append(points)# not iincluding background info.\n",
    "        #destroy all windows\n",
    "        cap.release()\n",
    "        cv.destroyAllWindows()\n",
    "        print(video_name, \" is done!!!\")\n",
    "        if data_for_folder is None:\n",
    "            data_for_folder = imputer(points_lst)\n",
    "            #print(data_for_folder.shape)\n",
    "        else:\n",
    "            data_for_folder = np.vstack((data_for_folder, imputer(points_lst)))\n",
    "            #print(data_for_folder.shape)\n",
    "        #print(video_name, \" is done!!!\")\n",
    "    # save the data points along with their labels...\n",
    "    if folder_path.split('_')[2] == \"backhand\":\n",
    "        np.savez_compressed(folder_path+\".npz\", data_for_folder, np.zeros((data_for_folder.shape[0],1)))\n",
    "    else:\n",
    "        np.savez_compressed(folder_path+\".npz\", data_for_folder, np.ones((data_for_folder.shape[0],1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "wicked-wireless",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.mp4  is done!!!\n",
      "11.mp4  is done!!!\n",
      "13.mp4  is done!!!\n",
      "20.mp4  is done!!!\n",
      "4.mp4  is done!!!\n",
      "1.mp4  is done!!!\n",
      "10.mp4  is done!!!\n",
      "12.mp4  is done!!!\n",
      "8.mp4  is done!!!\n",
      "6.mp4  is done!!!\n",
      "18.mp4  is done!!!\n",
      "3.mp4  is done!!!\n",
      "14.mp4  is done!!!\n",
      "16.mp4  is done!!!\n",
      "7.mp4  is done!!!\n",
      "15.mp4  is done!!!\n",
      "2.mp4  is done!!!\n",
      "17.mp4  is done!!!\n",
      "19.mp4  is done!!!\n",
      "9.mp4  is done!!!\n"
     ]
    }
   ],
   "source": [
    "# call this function to generate data and save it.\n",
    "folder_path = \"video_data/right_righty_forehand_shot\"\n",
    "body_points_generator(folder_path, \"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-thought",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-chuck",
   "metadata": {},
   "source": [
    "**DATA ENGINEERING (EDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "million-presence",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the data\n",
    "data1 = np.load(\"engineered_data/left_lefty_backhand_shot.npz\")\n",
    "data2 = np.load(\"engineered_data/left_lefty_forehand_shot.npz\")\n",
    "data3 = np.load(\"engineered_data/left_righty_backhand_shot.npz\")\n",
    "data4 = np.load(\"engineered_data/left_righty_forehand_shot.npz\")\n",
    "data11 = np.load(\"engineered_data/right_lefty_backhand_shot.npz\")\n",
    "data21 = np.load(\"engineered_data/right_lefty_forehand_shot.npz\")\n",
    "data31 = np.load(\"engineered_data/right_righty_backhand_shot.npz\")\n",
    "data41 = np.load(\"engineered_data/right_righty_forehand_shot.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecological-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extractor(data):\n",
    "    X, y = data[\"arr_0\"], data[\"arr_1\"]; del data; gc.collect()\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "based-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = data_extractor(data1)\n",
    "X2, y2 = data_extractor(data2)\n",
    "X3, y3 = data_extractor(data3)\n",
    "X4, y4 = data_extractor(data4)\n",
    "X5, y5 = data_extractor(data11)\n",
    "X6, y6 = data_extractor(data21)\n",
    "X7, y7 = data_extractor(data31)\n",
    "X8, y8 = data_extractor(data41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "patent-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try for lefty first\n",
    "X = np.vstack((X7, X8)); y = np.vstack((y7, y8))\n",
    "X_shuffled, y_shuffled = shuffle(X, y, random_state = 23)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "piano-eating",
   "metadata": {},
   "source": [
    "# splitting the data into train and test.\n",
    "trainX, valX, trainY, valY = train_test_split(X_shuffled, y_shuffled, test_size = 0.2, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-advertiser",
   "metadata": {},
   "source": [
    "***Scale the Data for Other Algorithms*** \\\n",
    "It doesn't help much, as all the datapoints fall within a particular range."
   ]
  },
  {
   "cell_type": "raw",
   "id": "unable-conducting",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import MinMaxScaler#, StandardScaler\n",
    "\n",
    "min_max = MinMaxScaler()\n",
    "trainX = min_max.fit_transform(trainX)\n",
    "valX = min_max.transform(valX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-uruguay",
   "metadata": {},
   "source": [
    "**Define all the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "continuing-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_scores(clf, fitted_model, trainX, trainY, valX, valY, X_shuffled, y_shuffled, cv=5):\n",
    "    print(\"Model score on Train set = \", fitted_model.score(trainX, trainY.squeeze()))\n",
    "    print(\"Model score on Val set = \", fitted_model.score(valX, valY.squeeze()))\n",
    "    cv_score1 = cross_val_score(clf, X_shuffled, y_shuffled.squeeze(), cv=cv)\n",
    "    print(\"5 Fold Cross-Validation Score for logreg = \", cv_score1)\n",
    "    print(\"Avg CV score = %.3f\"%np.mean(cv_score1))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-museum",
   "metadata": {},
   "source": [
    "###### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "electric-delivery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score on Train set =  1.0\n",
      "Model score on Val set =  0.75\n",
      "5 Fold Cross-Validation Score for logreg =  [1.   0.75 1.   1.   0.75 0.75 0.75 0.75 1.   1.  ]\n",
      "Avg CV score = 0.875\n"
     ]
    }
   ],
   "source": [
    "clf2 = LogisticRegression(C=0.01, n_jobs=-1)\n",
    "logreg = clf2.fit(trainX, trainY.squeeze())\n",
    "\n",
    "model_scores(clf2, logreg, trainX, trainY, valX, valY, X_shuffled, y_shuffled, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-museum",
   "metadata": {},
   "source": [
    "###### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "internal-marks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score on Train set =  1.0\n",
      "Model score on Val set =  0.75\n",
      "5 Fold Cross-Validation Score for logreg =  [1.   0.75 0.75 0.75 0.75 1.   0.5  0.75 1.   1.  ]\n",
      "Avg CV score = 0.825\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth = 7, n_jobs = -1, n_estimators = 50)\n",
    "rf = clf.fit(trainX, trainY.squeeze())\n",
    "\n",
    "model_scores(clf, rf, trainX, trainY, valX, valY, X_shuffled, y_shuffled, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-bottle",
   "metadata": {},
   "source": [
    "###### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "vocational-blowing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score on Train set =  1.0\n",
      "Model score on Val set =  0.75\n",
      "[21:31:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:31:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:31:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:31:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:32:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:32:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:32:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:32:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:32:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:32:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "5 Fold Cross-Validation Score for logreg =  [1.   1.   0.5  0.5  0.75 1.   0.75 0.75 1.   1.  ]\n",
      "Avg CV score = 0.825\n"
     ]
    }
   ],
   "source": [
    "clf3 = XGBClassifier(learning_rate = 0.1, n_estimators = 10, use_label_encoder = False, n_jobs=-1)\n",
    "xgb = clf3.fit(trainX, trainY.squeeze(), eval_metric = \"logloss\")\n",
    "\n",
    "model_scores(clf3, xgb, trainX, trainY, valX, valY, X_shuffled, y_shuffled, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-synthesis",
   "metadata": {},
   "source": [
    "###### K-Neighbours Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "designing-identification",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score on Train set =  0.875\n",
      "Model score on Val set =  0.75\n",
      "5 Fold Cross-Validation Score for logreg =  [1.   0.75 0.75 1.   1.   1.   0.5  0.75 1.   1.  ]\n",
      "Avg CV score = 0.875\n"
     ]
    }
   ],
   "source": [
    "clf1 = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "knn = clf1.fit(trainX, trainY.squeeze())\n",
    "\n",
    "model_scores(clf1, knn, trainX, trainY, valX, valY, X_shuffled, y_shuffled, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-scheduling",
   "metadata": {},
   "source": [
    "###### Multilayer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4 = MLPClassifier(hidden_layer_sizes=(450, 350), max_iter=150, learning_rate = \"adaptive\", learning_rate_init = 1e-5)\n",
    "mlp = clf4.fit(trainX, trainY.squeeze())\n",
    "\n",
    "model_scores(clf4, mlp, trainX, trainY, valX, valY, X_shuffled, y_shuffled, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-spanking",
   "metadata": {},
   "source": [
    "###### Extra-Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "accompanied-earth",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score on Train set =  1.0\n",
      "Model score on Val set =  0.875\n",
      "5 Fold Cross-Validation Score for logreg =  [1.   0.75 0.75 0.75 1.   1.   0.5  0.75 1.   1.  ]\n",
      "Avg CV score = 0.850\n"
     ]
    }
   ],
   "source": [
    "clf5 = ExtraTreesClassifier(max_depth = 5, n_jobs=-1, n_estimators = 130)\n",
    "etc = clf5.fit(trainX, trainY.squeeze())\n",
    "\n",
    "model_scores(clf5, etc, trainX, trainY, valX, valY, X_shuffled, y_shuffled, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-portugal",
   "metadata": {},
   "source": [
    "### Stacking Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "varying-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking_ensemble(model_members, X, y, cv=5):\n",
    "    skf = StratifiedKFold(n_splits = cv, shuffle=True, random_state=23)\n",
    "    val_pred = None; Y_test = np.empty((1,1), dtype = np.int8)\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        prediction = None\n",
    "        for model in model_members:\n",
    "            if str(model)[:3] == \"XGB\":\n",
    "                fitted_model = model.fit(X_train, y_train, eval_metric = \"logloss\")\n",
    "            else:\n",
    "                fitted_model = model.fit(X_train, y_train)\n",
    "            if prediction is None:\n",
    "                prediction = (fitted_model.predict_proba(X_test)[:, 1]).reshape(-1,1)\n",
    "            else:\n",
    "                prediction = np.hstack((prediction, (fitted_model.predict_proba(X_test)[:, 1]).reshape(-1, 1)))\n",
    "        if val_pred is None:\n",
    "            val_pred = prediction\n",
    "            #print(\"Prediction shape = \", prediction.shape)\n",
    "        else:\n",
    "            val_pred = np.vstack((val_pred, prediction))\n",
    "            #print(\"Val Prediction shape = \", val_pred.shape)\n",
    "        Y_test = np.concatenate((Y_test, y_test.reshape(-1, 1)), axis = 0)\n",
    "        print(\"Fold %d completed!!!\"%(i+1))\n",
    "    return val_pred, Y_test[1:].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "prescribed-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_lst = [LogisticRegression(C=0.01, n_jobs=-1),\n",
    "              RandomForestClassifier(max_depth = 7, n_jobs = -1, n_estimators = 50),\n",
    "              ExtraTreesClassifier(max_depth = 5, n_jobs=-1, n_estimators = 130),\n",
    "              MLPClassifier(hidden_layer_sizes=(450, 350), max_iter=170, learning_rate = \"adaptive\", learning_rate_init = 1e-5),\n",
    "              XGBClassifier(learning_rate = 0.1, n_estimators = 10, use_label_encoder = False, n_jobs=-1),\n",
    "              KNeighborsClassifier(n_neighbors=3, n_jobs=-1)]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "passive-brass",
   "metadata": {},
   "source": [
    "models_lst = [LogisticRegression(C=0.001, n_jobs=-1),\n",
    "              RandomForestClassifier(max_depth = 3, n_jobs = -1, n_estimators = 18),\n",
    "              ExtraTreesClassifier(max_depth = 3, n_jobs=-1, n_estimators = 130),\n",
    "              MLPClassifier(hidden_layer_sizes=(400, 350), max_iter=150, learning_rate = \"adaptive\", learning_rate_init = 1e-5),\n",
    "              XGBClassifier(learning_rate = 0.1, n_estimators = 10, use_label_encoder = False, n_jobs=-1),\n",
    "              KNeighborsClassifier(n_neighbors=3, n_jobs=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "explicit-vessel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 completed!!!\n",
      "Fold 2 completed!!!\n",
      "Fold 3 completed!!!\n",
      "Fold 4 completed!!!\n",
      "Fold 5 completed!!!\n"
     ]
    }
   ],
   "source": [
    "# obtain stacking data\n",
    "X_meta, y_meta = stacking_ensemble(models_lst, X_shuffled, y_shuffled.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "advance-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def members_score(members, X, y): return [np.mean(cross_validate(model, X, y, cv = 5, scoring = ('accuracy'))['test_score']) for model in members]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "id": "individual-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a meta-classifier that train on these predictions to output required results\n",
    "rf_meta = RandomForestClassifier(max_depth = 5, n_jobs = -1, n_estimators = 100)#.fit(val_stack_train, y_train)\n",
    "logreg_meta = LogisticRegression(C=1, n_jobs=-1)#.fit(val_stack_train, y_train)\n",
    "xgb_meta = XGBClassifier(learning_rate=1, n_estimators=10, use_label_encoder = False, n_jobs=-1)#.fit(val_stack_train, y_train, eval_metric = \"logloss\")\n",
    "mlp_meta = MLPClassifier(hidden_layer_sizes=(64, 50), max_iter=100, learning_rate = \"adaptive\", learning_rate_init = 1e-4)#.fit(val_stack_train, y_train)\n",
    "etc_meta = ExtraTreesClassifier(max_depth = 3, n_jobs=-1)#.fit(val_stack_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = members_score([rf_meta, logreg_meta, xgb_meta, mlp_meta, etc_meta], val_stack_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "id": "modern-retirement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of RandomForestClassifier is = 0.8500\n",
      "Score of LogisticRegression is = 0.8750\n",
      "Score of XGBClassifier is = 0.9000\n",
      "Score of MLPClassifier is = 0.7250\n",
      "Score of ExtraTreesClassifier is = 0.9000\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip([rf_meta, logreg_meta, xgb_meta, mlp_meta, etc_meta], scores):\n",
    "    print(\"Score of %s is = %.4f\"%(str(i).split('(')[0], j))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-aurora",
   "metadata": {},
   "source": [
    "# Training the best models on full dataset to launch it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "lined-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model_saver(models_lst, meta_model, X, y, X_meta, y_meta):\n",
    "    video_side = input(\"Enter video side: \"); hand_type = input(\"Enter hand type: \")\n",
    "    for model in models_lst:\n",
    "        if str(model)[:3] == \"XGB\":\n",
    "            fitted_model = model.fit(X, y, eval_metric = \"logloss\")\n",
    "        else:\n",
    "            fitted_model = model.fit(X, y)\n",
    "        model_name = str(model).split('(')[0]\n",
    "        pickle.dump(fitted_model, open(os.path.join(\"best_models\", video_side+'_'+hand_type, model_name+\".sav\"), 'wb'))\n",
    "    if str(meta_model)[:3] == \"XGB\":\n",
    "        fitted_meta_model = meta_model.fit(X_meta, y_meta, eval_metric = \"logloss\")\n",
    "    else:\n",
    "        fitted_meta_model = meta_model.fit(X_meta, y_meta)\n",
    "    pickle.dump(fitted_meta_model, open(os.path.join(\"meta_models\", video_side+'_'+hand_type+'_'+\"meta.sav\"), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "empirical-compromise",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = XGBClassifier(learning_rate=1, n_estimators=10, use_label_encoder = False, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "assumed-wholesale",
   "metadata": {},
   "source": [
    "models_lst = [LogisticRegression(C=0.001, n_jobs=-1),\n",
    "              RandomForestClassifier(max_depth = 5, n_jobs = -1, n_estimators = 70),\n",
    "              ExtraTreesClassifier(max_depth = 3, n_jobs=-1, n_estimators = 120),\n",
    "              MLPClassifier(hidden_layer_sizes=(450, 400), max_iter=150),\n",
    "              XGBClassifier(learning_rate = 0.1, n_estimators = 3, use_label_encoder = False, n_jobs=-1),\n",
    "              KNeighborsClassifier(n_neighbors=3, n_jobs=-1)]\n",
    "\n",
    "#XGBClassifier(learning_rate=0.1, n_estimators=3, use_label_encoder = False, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "matched-ethnic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter video side: right\n",
      "Enter hand type: righty\n"
     ]
    }
   ],
   "source": [
    "# Call the function above to save the models...\n",
    "best_model_saver(models_lst, meta_model, X_shuffled, y_shuffled.squeeze(), X_meta, y_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-content",
   "metadata": {},
   "source": [
    "## Full pipeline for video-classification with OpenPose to predict whether it is backhand or forehand."
   ]
  },
  {
   "cell_type": "raw",
   "id": "lucky-combat",
   "metadata": {},
   "source": [
    "model_name = 'MLP_clf_rodo_res50.sav'\n",
    "pickle.dump(classifier, open(path1+model_name, 'wb'))\n",
    "# first load the model...\n",
    "model_name = 'MLP_clf_rodo_res50.sav'\n",
    "clf_model_trained = pickle.load(open(path1+model_name, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "professional-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loader(video_side, hand_type):\n",
    "    models_lst = [pickle.load(open(os.path.join(\"best_models\", video_side+'_'+hand_type, \"\", model_name), 'rb')) for model_name in os.listdir(os.path.join(\"best_models\", video_side+'_'+hand_type, \"\"))]\n",
    "    meta_model = pickle.load(open(\"meta_models/\"+video_side+\"_\"+hand_type+\"_\"+\"meta.sav\", 'rb'))\n",
    "    return models_lst, meta_model\n",
    "\n",
    "def tup_unpack(tup): return [tup[0], tup[1]]\n",
    "\n",
    "def frames_unraveler(points_lst):\n",
    "    data = None\n",
    "    for i in points_lst:\n",
    "        data_new = [tup_unpack(j) if j is not None else tup_unpack((np.nan,np.nan)) for j in i]\n",
    "        data_new = np.array(data_new, dtype = np.float32).ravel()\n",
    "        if data is None:\n",
    "            data = data_new\n",
    "        else:\n",
    "            data = np.vstack((data, data_new))\n",
    "    return data\n",
    "\n",
    "def imputer(points_lst):\n",
    "    data = frames_unraveler(points_lst)\n",
    "    #import sklearn.preprocessing.Impute\n",
    "    imp = SimpleImputer(missing_values = np.nan, strategy = \"most_frequent\")#\"mean\"\n",
    "    imp.fit(data); data = imp.transform(data)\n",
    "    return data.reshape((1, -1)).squeeze()\n",
    "\n",
    "def body_keypoints(video_path, video_side):\n",
    "    net  = cv.dnn.readNetFromTensorflow(\"graph_opt.pb\")\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "    cap.set(cv.CAP_PROP_FPS, 10)\n",
    "    cap.set(3, 800)\n",
    "    cap.set(4, 800)\n",
    "    # 'Threshold value for pose parts heat map'\n",
    "    thr = 0.2#<-----------------------------------------------------------\n",
    "    # 'Resize input to specific width.'\n",
    "    width = 368\n",
    "    # 'Resize input to specific height.'\n",
    "    height = 368\n",
    "    # if video is not opened\n",
    "    if not cap.isOpened():\n",
    "        cap = cv.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open video\")\n",
    "    flag = True; points_lst = []; ctr = 0\n",
    "    while cv.waitKey(1) < 0 and ctr < 20:\n",
    "        hasFrame, frame = cap.read(); ctr += 1\n",
    "        if not hasFrame:\n",
    "            cv.waitKey()\n",
    "            break\n",
    "        #if cv.getWindowProperty('crop_frame', cv.WND_PROP_VISIBLE) < 1:\n",
    "        #    break\n",
    "        if cv.waitKey(10) & 0xFF == ord('q') :\n",
    "            # break out of the while loop\n",
    "            break\n",
    "        if video_side == \"left\":\n",
    "            crop_frame = frame[:, 0:700, :]\n",
    "        else:\n",
    "            crop_frame = frame[:, 700:, :]\n",
    "        crop_frameWidth = crop_frame.shape[1]\n",
    "        crop_frameHeight = crop_frame.shape[0]\n",
    "        inp = cv.dnn.blobFromImage(crop_frame, 1.0, (width, height), (127.5, 127.5, 127.5), swapRB=False, crop=False)#inScale\n",
    "        net.setInput(inp)\n",
    "        out = net.forward()\n",
    "        out = out[:, :19, :, :]\n",
    "        #assert(len(BODY_PARTS) <= out.shape[1])\n",
    "        points = []\n",
    "        required_body_points = [0, 1, 2, 3, 4, 5, 6, 7, 8, 11]# see BODY_PARTS_M for reference...\n",
    "        for i in required_body_points:\n",
    "            # Slice heatmap of corresponding body's part.\n",
    "            heatMap = out[0, i, :, :]\n",
    "            # Originally, we try to find all the local maximums. To simplify a sample\n",
    "            # we just find a global one. However only a single pose at the same time\n",
    "            # could be detected this way.\n",
    "            _, conf, _, point = cv.minMaxLoc(heatMap)\n",
    "            x = (crop_frameWidth * point[0]) / out.shape[3]\n",
    "            y = (crop_frameHeight * point[1]) / out.shape[2]\n",
    "            # Add a point if it's confidence is higher than threshold.\n",
    "            points.append((int(x), int(y)) if conf > thr else None)\n",
    "        points_lst.append(points)# not including background info.\n",
    "    #destroy all windows\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()\n",
    "    return imputer(points_lst)\n",
    "\n",
    "def predictor(data, models_lst, meta_model):\n",
    "    pred_array = np.array([model.predict_proba(data.reshape(1, -1))[0,1] for model in models_lst]).reshape(1, -1)\n",
    "    #print(\"Pred array = \", pred_array)\n",
    "    return meta_model.predict(pred_array)\n",
    "\n",
    "def vid_classifier(video_path, video_side, hand_type):\n",
    "    body_points = body_keypoints(video_path, video_side)\n",
    "    models_lst, meta_model = model_loader(video_side, hand_type)\n",
    "    pred_value = predictor(body_points, models_lst, meta_model)\n",
    "    if pred_value:\n",
    "        print(\"\\n\\nVideo of Forehand Short\\n\\n\")\n",
    "    else:\n",
    "        print(\"\\n\\nVideo of Backhand Shot\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bridal-clear",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your video path (provide the absolute path if saved in other directories else provide relative path): left_lefty_backhand_shot/2.mp4\n",
      "Is it left or right? Type (left/right): left\n",
      "Is player a lefty or righty? (lefty/righty): lefty\n",
      "\n",
      "\n",
      "Video of Backhand Shot\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "video_path = input(\"Enter your video path (provide the absolute path if saved in other directories else provide relative path): \")\n",
    "video_side = input(\"Is it left or right? Type (left/right): \")\n",
    "hand_type = input(\"Is player a lefty or righty? (lefty/righty): \")\n",
    "vid_classifier(video_path, video_side.lower(), hand_type.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "played-roulette",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 6)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "blocked-producer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "geological-drinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = np.array([1,2,3]).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "intermediate-prime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-crawford",
   "metadata": {},
   "source": [
    "###### Rough"
   ]
  },
  {
   "cell_type": "raw",
   "id": "interested-cylinder",
   "metadata": {},
   "source": [
    "data = imputer(points_lst)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "competitive-custom",
   "metadata": {},
   "source": [
    "for i in os.listdir(\"left_lefty_backhand_shot\"):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "invalid-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = frames_unraveler(points_lst)\n",
    "#import sklearn.preprocessing.Impute\n",
    "imp = SimpleImputer(missing_values = np.nan, strategy = \"most_frequent\")#\"mean\"\n",
    "imp.fit(data1); data = imp.transform(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "grand-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "net  = cv.dnn.readNetFromTensorflow(\"graph_opt.pb\")\n",
    "cap = cv.VideoCapture(\"right_lefty_forehand_shot/5.mp4\")\n",
    "cap.set(cv.CAP_PROP_FPS, 10)\n",
    "cap.set(3, 800)\n",
    "cap.set(4, 800)\n",
    "# 'Threshold value for pose parts heat map'\n",
    "thr = 0.2#<-----------------------------------------------------------\n",
    "# 'Resize input to specific width.'\n",
    "width = 368\n",
    "# 'Resize input to specific height.'\n",
    "height = 368\n",
    "# if video is not opened\n",
    "if not cap.isOpened():\n",
    "    cap = cv.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open video\")\n",
    "\n",
    "flag = True; points_lst = []; ctr = 0\n",
    "while cv.waitKey(1) < 0 and ctr < 20:# \n",
    "    hasFrame, frame = cap.read(); ctr += 1\n",
    "    if not hasFrame:\n",
    "        cv.waitKey()\n",
    "        break\n",
    "    #if cv.getWindowProperty('crop_frame', cv.WND_PROP_VISIBLE) < 1:\n",
    "    #    break\n",
    "    if cv.waitKey(10) & 0xFF == ord('q') :\n",
    "        # break out of the while loop\n",
    "        break\n",
    "    crop_frame = frame[:, 700:, :]\n",
    "    crop_frameWidth = crop_frame.shape[1]\n",
    "    crop_frameHeight = crop_frame.shape[0]\n",
    "    inp = cv.dnn.blobFromImage(crop_frame, 1.0, (width, height), (127.5, 127.5, 127.5), swapRB=False, crop=False)#inScale#127.5, 127.5, 127.5\n",
    "    net.setInput(inp)\n",
    "    out = net.forward()\n",
    "    out = out[:, :19, :, :]\n",
    "    #print(out.shape[1])\n",
    "    assert(len(BODY_PARTS_C) <= out.shape[1])\n",
    "\n",
    "    points = []\n",
    "    required_body_points = [0, 1, 2, 3, 4, 5, 6, 7, 8, 11]# see BODY_PARTS_M for reference...\n",
    "    for i in range(len(BODY_PARTS_C)):\n",
    "        # Slice heatmap of corresponding body's part.\n",
    "        heatMap = out[0, i, :, :]\n",
    "\n",
    "        # Originally, we try to find all the local maximums. To simplify a sample\n",
    "        # we just find a global one. However only a single pose at the same time\n",
    "        # could be detected this way.\n",
    "        _, conf, _, point = cv.minMaxLoc(heatMap)\n",
    "        x = (crop_frameWidth * point[0]) / out.shape[3]\n",
    "        y = (crop_frameHeight * point[1]) / out.shape[2]\n",
    "\n",
    "        # Add a point if it's confidence is higher than threshold.\n",
    "        points.append((int(x), int(y)) if conf > thr else None)\n",
    "    tmp_lst = [kk for kk in points[:9]]; tmp_lst.append(points[11])\n",
    "    points_lst.append(tmp_lst)\n",
    "    for pair in POSE_PAIRS_C_new:\n",
    "        partFrom = pair[0]\n",
    "        partTo = pair[1]\n",
    "        assert(partFrom in BODY_PARTS_C)\n",
    "        assert(partTo in BODY_PARTS_C)\n",
    "\n",
    "        idFrom = BODY_PARTS_C[partFrom]\n",
    "        idTo = BODY_PARTS_C[partTo]\n",
    "\n",
    "        if points[idFrom] and points[idTo]:\n",
    "            cv.line(crop_frame, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
    "            cv.ellipse(crop_frame, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "            cv.ellipse(crop_frame, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "\n",
    "    t, _ = net.getPerfProfile()\n",
    "    freq = cv.getTickFrequency() / 1000\n",
    "    cv.putText(crop_frame, '%.2fms' % (t / freq), (10, 20), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0))\n",
    "\n",
    "    cv.imshow('OpenPose using OpenCV', crop_frame)\n",
    "#destroy all windows\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for pair in POSE_PAIRS_M:\n",
    "    partFrom = pair[0]\n",
    "    partTo = pair[1]\n",
    "    assert(partFrom in BODY_PARTS_M)\n",
    "    assert(partTo in BODY_PARTS_M)\n",
    "\n",
    "    idFrom = BODY_PARTS_M[partFrom]\n",
    "    idTo = BODY_PARTS_M[partTo]\n",
    "\n",
    "    if points[idFrom] and points[idTo]:\n",
    "        cv.line(crop_frame, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
    "        cv.ellipse(crop_frame, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "        cv.ellipse(crop_frame, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "\n",
    "t, _ = net.getPerfProfile()\n",
    "freq = cv.getTickFrequency() / 1000\n",
    "cv.putText(crop_frame, '%.2fms' % (t / freq), (10, 20), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0))\n",
    "\n",
    "cv.imshow('OpenPose using OpenCV', crop_frame)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
